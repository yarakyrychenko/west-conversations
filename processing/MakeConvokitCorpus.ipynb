{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7192,"status":"ok","timestamp":1650507159783,"user":{"displayName":"Yara Kyrychenko","userId":"18407843692735870383"},"user_tz":240},"id":"jknO8bcDuamq","outputId":"31576c3b-a0d9-49e2-d2f6-56825c9c4d12"},"outputs":[],"source":["# import necessary modules\n","from convokit import Corpus, TextCleaner\n","import numpy as np, pandas as pd, os"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Change the path, csv name and study code as appropriate\n","path = os.getcwd()[:-10] # the path to the github repo\n","master_csv = 'master.csv'\n","STUDY_CODE = 'AD'\n","# read in the data\n","df = pd.read_csv(path+'data/raw/'+master_csv)\n","# rename and drop some columns\n","df = df.rename(columns={\"Participant ID\": \"Person1\", \"Partner ID\": \"Person2\", \"Conversation type \": \"ConvoType\", \"Group ID\": \"GroupID\"}).drop(df.columns[6:],axis=1).drop(df.columns[0],axis=1)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# drop all rows without Speaker1 determination\n","df = df.drop(df[df.Speaker1.isna()].index)\n","# standardize columns \n","df.Speaker1 = df.Speaker1.astype(\"int64\")\n","df.ConvoType = df.ConvoType.apply(lambda x: x.lower())\n","df.GroupID = df.GroupID.apply(lambda x: x.lower())\n","# add \"Text\" column\n","df['Text'] = [None]*len(df)\n","# reset index\n","df = df.reset_index()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def read_file(file_path: str) -> str:\n","    \"\"\"\n","    Reads a txt file in, replaces characters to standardize, and returns text.\n","        in : str file path (.txt)\n","        out: str text of the file\n","    \"\"\"\n","\n","    # open the file and read it in\n","    text_file = open(file_path, \"r\")\n","    text = text_file.read()\n","    text_file.close()\n","    # replace things \n","    text = text.replace(\"Speaker 1\", \"S1\")\n","    text = text.replace(\"Speaker 2\", \"S2\")\n","    text = text.replace(\"\\ufeff\", \"\")\n","    return text\n","\n","def add_text(dataframe, convotype, folderpath):\n","    \"\"\"\n","    Takes in the master dataframe, and a name of conversation and folder to read data from. \n","    Adds the text of files to the dataframe in appropriate places (inplace). \n","    As the result, the dataframe will have the \"Text\" column filled out for that conversation type. \n","        in : \n","            pd.DataFrame\n","            str name of the type of conversations\n","            str name of the folderpath where the conversations of this type are (e.g. STUDYCODE/Foldername)\n","        out: nothing\n","    \"\"\"\n","\n","    # for each row of the dataframe\n","    for index, row in dataframe.iterrows():\n","        # if the row's ConvoType is the same as the current convotype\n","        if row.ConvoType.startswith(convotype):\n","            # read in the txt file corresponding to the row\n","            try:\n","                file_path = f'{path}data/raw/transcripts/{folderpath}/{STUDY_CODE}_{row.Person1}_{row.Person2}_{row.ConvoType}_{row.GroupID}.txt'\n","                text = read_file(file_path) \n","            except:\n","                file_path = f'{path}data/raw/transcripts/{folderpath}/{STUDY_CODE}_{row.Person2}_{row.Person1}_{row.ConvoType}_{row.GroupID}.txt'\n","                text = read_file(file_path) \n","            # add the text to the \"Text\" column in the dataframe \n","            dataframe.loc[index, \"Text\"] = text\n","\n","def preproc(text: str) -> list:\n","    \"\"\"\n","    Preprocess text from txt files by separating it into utterances (speaker lines)and standardizing \"no one's\" lines.\n","    in : str some text\n","    out: list of strings each of which is a line spoken by a person\n","    \"\"\"\n","\n","    # make the text into a list of lines by splitting on new line (\\n)\n","    text = text.split(\"\\n\\n\\n\") if len(text.split(\"\\n\\n\\n\")) > 1 else text.split(\"\\n\\n\")\n","    # for each line\n","    to_remove = []\n","    for i in range(len(text)):\n","        line = text[i]\n","        # if line's speaker is not S1 AND not S2 \n","        if line[10:12] != 'S1' and line[10:12] != 'S2':\n","            # combine that line with the previous line and add \"[both]\" to separate them \n","            text[i-1] = text[i-1] + \" [both] \" + line\n","            # add that line to a list of lines to be removed\n","            to_remove.append(i)\n","    # remove the lines that need to be removed\n","    for item in sorted(to_remove, reverse=True):\n","        text.pop(item)\n","    return text"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# change the names (first=convotype,second=foldername)\n","add_text(df,\"intro\",\"Introduction\")\n","add_text(df,\"nego1\",\"Negotiation1\")\n","add_text(df,\"nego2\",\"Negotiation2\")\n","add_text(df,\"nego3\",\"Negotiation3\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/_t/3tgpl7zn439gpfzt3h2g1q2m0000gn/T/ipykernel_64759/3975918380.py:55: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n","  utterances_df = temp_df.drop([\"start\"],1)\n"]}],"source":["# this cell creates an utterances_df dataframe that will then be converted into a ConvokitCorpus\n","# initialize a temp dataframe\n","colsnewdf = ['speaker', 'conversation_id', 'text', 'timestamp', 'start']\n","temp_df = pd.DataFrame(columns = colsnewdf)\n","# initialize lists for debugging\n","length, conversation_ids = [], []\n","# set paramenter n = minimum number of lines a conversation has to have to be included in the corpus\n","# (just keep it 0)\n","n = 0\n","\n","for i in range(len(df)):\n","    # preproces text from the i'th row of the master dataframe to get a list of lines\n","    listoflines = preproc(df.loc[i,\"Text\"])\n","    length.append(len(listoflines))\n","    # get conversation id for the conversation consisting of ConvoType and GroupID\n","    conversation_id = df.loc[i,\"ConvoType\"] + \"_\" + df.loc[i,\"GroupID\"]\n","    conversation_ids.append(conversation_id)\n","    # initialize temp list to hold utternaces and their metadata to be added to the temp_df later on\n","    list1 = [] \n","    # if there are more than zero lines in the conversation\n","    if len(listoflines) > n:\n","      # determine the speaker ids \n","      speakerA = df.loc[i, \"Speaker1\"]\n","      speakerB = df.loc[i, \"Person2\"] if speakerA == df.loc[i, \"Person1\"] else df.loc[i, \"Person1\"]\n","\n","      # for each line in the conversation\n","      for j in range(len(listoflines)):\n","        # if it's the first line, set start variable to True to mark the begining of conversation\n","        start = True if j == 0 else False\n","        # get the current line\n","        line = listoflines[j]\n","        # if the current line was said by the first speaker (S1)\n","        if line[10:12] == \"S1\":\n","          # add the data about it to the temp list in the order of colsnewdf (speaker = speakerA)\n","          #   ['speaker', 'conversation_id', 'text', 'timestamp', 'start']\n","          # 'text' = characters in the line starting after the timestamp\n","          # 'timestamp' = characters in the line starting before the speaker 'S1/2' and 'text'\n","          list1.append([speakerA, conversation_id, line[14:], line[:9], start])\n","        if line[10:12] == \"S2\":\n","          # add the data about it to the temp list in the order of colsnewdf (speaker = speakerB)\n","          list1.append([speakerB, conversation_id, line[14:], line[:9], start] )\n","      # turn the temp list into another temp dataframe\n","      df1 = pd.DataFrame(list1, columns=colsnewdf)\n","      # merge the temp dataframes\n","      temp_df = pd.concat([temp_df, df1], ignore_index=True)\n","\n","# add an (utterance) id column to the temp dataframe\n","# utterance ids are just the index of the utterance plus one turned into a string (str)\n","temp_df['id'] = [str(i+1) for i in temp_df.index]\n","# add a 'reply_to' column to the temp dataframe \n","# it's just the previous line in our case (or None if that's the first line) \n","reply_to = [str(i) for i in temp_df.index]\n","temp_df['reply_to'] = np.where(temp_df.start, None, reply_to)\n","# turn the temp dataframe into the final utterances_df by dropping the \"start\" column\n","utterances_df = temp_df.drop([\"start\"],1)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["5539it [00:00, 25401.85it/s]\n"]}],"source":["# turn the utterances_df dataframe into a convokit corpus\n","corpus=Corpus.from_pandas(utterances_df)\n","# this ensures that each utteraces has a speaker (object) associated with it (instead of just speaker id)\n","for utt in corpus.iter_utterances():\n","    utt.speaker = corpus.speakers[utt.speaker.id]"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of Speakers: 73\n","Number of Utterances: 5539\n","Number of Conversations: 103\n"]}],"source":["# print statistics for the corpus\n","corpus.print_summary_stats()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["SPEAKERS\n","The number of speakers in the csv is 73.\n","The number of speakers in the corpus is 73.\n","Speakers set() are in the coprus but not the csv.\n","Speakers set() are in the csv but not the corpus.\n","\n","CONVOS\n","There are 103 conversations in the csv.\n","There are 103 in the corpus.\n","Conversations with ids [] have 0 lines.\n","The conversations with 0 lines are:\n","None\n"]}],"source":["# SAFETY CHECKS\n","\n","# you can set the number of lines to see how many conversations with that number of lines are in the corpus\n","num_lines = 0\n","\n","# Speakers \n","# if everything is fine you shouls see the same number of speakers in the csv as in the corpus\n","print(\"SPEAKERS\")\n","original_speakers = set(df.Person1.astype(str)).union( set(df.Person2.astype(str)) )\n","new_speakers = set([spkr.id for spkr in corpus.iter_speakers()])\n","print(f\"The number of speakers in the csv is {len(original_speakers)}.\\nThe number of speakers in the corpus is {len(new_speakers)}.\")\n","print(f\"Speakers {new_speakers - original_speakers} are in the coprus but not the csv.\")\n","print(f\"Speakers {original_speakers - new_speakers} are in the csv but not the corpus.\")\n","\n","# Convos\n","# if everything is fine you shouls see the same number of conversations in the csv as in the corpus\n","print(\"\\nCONVOS\")\n","n_liners = [ i for i in range(len(length)) if length[i] == num_lines ]\n","print(f\"There are {len(df)} conversations in the csv.\\nThere are {len(corpus.conversations)} in the corpus.\")\n","print(f\"Conversations with ids {n_liners} have {num_lines} lines.\")\n","for convo in corpus.iter_conversations():\n","    if len(convo._utterance_ids) == num_lines:\n","        print(convo.id)\n","print(f\"The conversations with {num_lines} lines are:\")\n","df.iloc[n_liners] if len(n_liners) > 0 else print(\"None\")"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# Might want to clean the text: \n","#   uncomment the code below to apply the built-in convokit cleaner and fix contractions (\"can't\" -> \"can not\") \n","\n","#cleaner = TextCleaner()\n","#corpus = cleaner.transform(corpus) \n","#import contractions\n","#for utt in corpus.iter_utterances():\n","#    tokens = utt.text.split()\n","#    text = [contractions.fix(word) for word in tokens]\n","#    utt.text = \" \".join(text)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["corpus.dump(name=\"corpus\", base_path=path+'data/processed/')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOfOu54Bb9b7/MA6AYXe9jc","name":"Study 3 Sentiment Analysis.ipynb","provenance":[]},"interpreter":{"hash":"db00b3a626384bec78137815b78dbfa3868ae3c424ba9ba9cfb25bba6eee4913"},"kernelspec":{"display_name":"Python 3.9.7 ('ml')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
